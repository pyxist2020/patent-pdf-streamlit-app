import streamlit as st
import json
import os
import tempfile
from pathlib import Path
from pypdf import PdfReader, PdfWriter
from typing import Dict, List
from patent_extractor import PatentExtractor

# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="ç‰¹è¨±PDFæ§‹é€ åŒ–ãƒ„ãƒ¼ãƒ«", page_icon="ğŸ“„", layout="wide")

# ã‚«ã‚¹ã‚¿ãƒ CSS
st.markdown("""
<style>
    .stat-box { padding: 10px; border-radius: 5px; margin-bottom: 10px; text-align: center; }
    .blue-stat { background-color: #f0f5ff; border: 1px solid #d0e0ff; }
    .green-stat { background-color: #f0fff5; border: 1px solid #d0ffe0; }
    .orange-stat { background-color: #fff5f0; border: 1px solid #ffe0d0; }
    .page-badge { display: inline-block; padding: 4px 8px; margin: 2px; border-radius: 12px; 
                  font-size: 0.8em; font-weight: 500; text-align: center; }
    .page-success { background-color: #d4edda; color: #155724; border: 1px solid #c3e6cb; }
    .page-error { background-color: #f8d7da; color: #721c24; border: 1px solid #f5c6cb; }
    .page-processing { background-color: #d1ecf1; color: #0c5460; border: 1px solid #bee5eb; }
</style>
""", unsafe_allow_html=True)

# ãƒ¢ãƒ‡ãƒ«ã‚ªãƒ—ã‚·ãƒ§ãƒ³
DEFAULT_MODELS = {
    "Google Gemini": ["gemini-1.5-pro", "gemini-1.5-flash"],
    "OpenAI": ["gpt-4o", "gpt-4o-mini"],
    "Anthropic": ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"]
}

@st.cache_data(ttl=3600)
def get_models(provider, api_key):
    """åˆ©ç”¨å¯èƒ½ãªãƒ¢ãƒ‡ãƒ«ã‚’å–å¾—"""
    if not api_key:
        return DEFAULT_MODELS.get(provider, [])
    try:
        if provider == "Google Gemini":
            import google.generativeai as genai
            genai.configure(api_key=api_key)
            return [m.name.replace('models/', '') for m in genai.list_models() 
                   if 'generateContent' in m.supported_generation_methods]
        elif provider == "OpenAI":
            from openai import OpenAI
            models = OpenAI(api_key=api_key).models.list()
            return sorted([m.id for m in models.data if 'gpt' in m.id.lower()], reverse=True)
        elif provider == "Anthropic":
            return ["claude-3-opus-20240229", "claude-3-sonnet-20240229", "claude-3-haiku-20240307"]
    except:
        pass
    return DEFAULT_MODELS.get(provider, [])

def load_schema(file_path=None, content=None):
    """JSONã‚¹ã‚­ãƒ¼ãƒã‚’èª­ã¿è¾¼ã¿"""
    try:
        if file_path and os.path.exists(file_path):
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        elif content:
            return json.loads(content)
    except Exception as e:
        st.error(f"ã‚¹ã‚­ãƒ¼ãƒèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
    return {}

def split_pdf_chunks(pdf_path, chunk_size=2, overlap=1):
    """PDFã‚’é‡è¤‡ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²"""
    chunks = []
    try:
        reader = PdfReader(pdf_path)
        total_pages = len(reader.pages)
        
        for i, start in enumerate(range(0, total_pages, chunk_size - overlap)):
            end = min(start + chunk_size, total_pages)
            writer = PdfWriter()
            
            for page_idx in range(start, end):
                writer.add_page(reader.pages[page_idx])
            
            temp_path = tempfile.mktemp(suffix=f"_chunk_{i+1}.pdf")
            with open(temp_path, "wb") as f:
                writer.write(f)
            
            chunks.append({
                "id": i + 1,
                "path": temp_path,
                "pages": list(range(start + 1, end + 1)),
                "start": start + 1,
                "end": end
            })
    except Exception as e:
        st.error(f"PDFåˆ†å‰²ã‚¨ãƒ©ãƒ¼: {e}")
    return chunks

def process_chunk(chunk, model_name, api_key, schema, prompt=None, temperature=0.1, max_tokens=32768):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆGeminiå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿å¯¾ç­–ï¼‰"""
    try:
        # ã‚ˆã‚Šå®‰å…¨ãªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½œæˆ
        safe_prompt = create_safe_prompt(prompt, schema)
        
        extractor = PatentExtractor(model_name, api_key, schema, safe_prompt, temperature, max_tokens)
        
        # Geminiã®å®‰å…¨æ€§è¨­å®šã‚’èª¿æ•´ã™ã‚‹å ´åˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        # (patent_extractorãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã‚µãƒãƒ¼ãƒˆã—ã¦ã„ã‚‹å ´åˆ)
        raw_result = extractor.process_patent_pdf(chunk["path"])
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
        if raw_result is None or raw_result == {}:
            # ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å ´åˆã¯åŸºæœ¬æƒ…å ±ã®ã¿æŠ½å‡º
            fallback_result = create_fallback_extraction(chunk)
            return {"id": chunk["id"], "pages": chunk["pages"], "status": "partial", 
                   "data": fallback_result, "warning": "å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã®ãŸã‚éƒ¨åˆ†çš„ãªæŠ½å‡º"}
        
        # JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_result = clean_json_response(raw_result)
        
        return {"id": chunk["id"], "pages": chunk["pages"], "status": "success", "data": cleaned_result}
        
    except Exception as e:
        error_msg = str(e)
        
        # Geminiç‰¹æœ‰ã®ã‚¨ãƒ©ãƒ¼ã‚’ç‰¹å®š
        if "finish_reason is 2" in error_msg or "SAFETY" in error_msg:
            # å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚‹ãƒ–ãƒ­ãƒƒã‚¯ã®å ´åˆ
            fallback_result = create_fallback_extraction(chunk)
            return {"id": chunk["id"], "pages": chunk["pages"], "status": "blocked", 
                   "data": fallback_result, "error": "å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯"}
        elif "response.text" in error_msg:
            # Geminiãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼ã®å ´åˆ
            return {"id": chunk["id"], "pages": chunk["pages"], "status": "error", 
                   "error": "Gemini APIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚¨ãƒ©ãƒ¼", "data": {}}
        else:
            # ãã®ä»–ã®ã‚¨ãƒ©ãƒ¼
            return {"id": chunk["id"], "pages": chunk["pages"], "status": "error", 
                   "error": error_msg, "data": {}}

def create_safe_prompt(original_prompt, schema):
    """å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã‚’å›é¿ã™ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ"""
    safe_prompt = f"""
    ä»¥ä¸‹ã¯å­¦è¡“çš„ãªç‰¹è¨±æ–‡æ›¸ã®æƒ…å ±æŠ½å‡ºã‚¿ã‚¹ã‚¯ã§ã™ã€‚
    
    ã‚¿ã‚¹ã‚¯: ç‰¹è¨±æ–‡æ›¸ã‹ã‚‰æ§‹é€ åŒ–ã•ã‚ŒãŸæƒ…å ±ã‚’æŠ½å‡ºã—ã€JSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚
    
    æŠ½å‡ºå¯¾è±¡:
    - ç‰¹è¨±ç•ªå·ã‚„å‡ºé¡˜æƒ…å ±
    - æŠ€è¡“åˆ†é‡
    - ç™ºæ˜ã®èƒŒæ™¯
    - èª²é¡Œã¨è§£æ±ºæ‰‹æ®µ
    - åŠ¹æœ
    - å®Ÿæ–½ä¾‹
    
    å‡ºåŠ›å½¢å¼: æœ‰åŠ¹ãªJSONã®ã¿
    
    æ³¨æ„äº‹é …:
    - å­¦è¡“çš„ãƒ»æ•™è‚²çš„ç›®çš„ã§ã®ä½¿ç”¨
    - å…¬é–‹ã•ã‚Œã¦ã„ã‚‹ç‰¹è¨±æƒ…å ±ã®æ§‹é€ åŒ–
    - ç ”ç©¶ç›®çš„ã§ã®æƒ…å ±æ•´ç†
    
    {original_prompt or ""}
    """
    
    return safe_prompt

def create_fallback_extraction(chunk):
    """å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒ–ãƒ­ãƒƒã‚¯æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æŠ½å‡º"""
    return {
        "processing_info": {
            "chunk_id": chunk["id"],
            "pages": chunk["pages"],
            "status": "limited_extraction",
            "reason": "å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã®ãŸã‚åŸºæœ¬æƒ…å ±ã®ã¿æŠ½å‡º"
        },
        "basic_info": {
            "document_type": "ç‰¹è¨±æ–‡æ›¸",
            "pages_processed": len(chunk["pages"]),
            "extraction_level": "minimal"
        }
    }

def handle_gemini_safety_settings():
    """Geminiå®‰å…¨æ€§è¨­å®šã®ã‚¬ã‚¤ãƒ€ãƒ³ã‚¹"""
    return """
    Geminiå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿å¯¾ç­–:
    
    1. ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®èª¿æ•´
       - å­¦è¡“çš„ãƒ»æ•™è‚²çš„ç›®çš„ã‚’æ˜è¨˜
       - æŠ€è¡“æ–‡æ›¸ã¨ã—ã¦æ‰±ã†ã“ã¨ã‚’å¼·èª¿
       
    2. ãƒ¢ãƒ‡ãƒ«é¸æŠ
       - gemini-1.5-flash ã‚ˆã‚Š gemini-1.5-pro ã‚’æ¨å¥¨
       - ã‚ˆã‚Šé«˜åº¦ãªã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆç†è§£
       
    3. å‡¦ç†æ–¹æ³•
       - ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã‚’å°ã•ã (1-2ãƒšãƒ¼ã‚¸)
       - è¤‡é›‘ãªå†…å®¹ã‚’åˆ†å‰²å‡¦ç†
    """

def merge_chunks_with_safety_handling(chunk_results):
    """å®‰å…¨æ€§ãƒ–ãƒ­ãƒƒã‚¯ã‚’è€ƒæ…®ã—ãŸãƒãƒ£ãƒ³ã‚¯çµ±åˆ"""
    successful = [r for r in chunk_results if r["status"] in ["success", "partial"]]
    blocked = [r for r in chunk_results if r["status"] == "blocked"]
    failed = [r for r in chunk_results if r["status"] == "error"]
    
    if not successful:
        return {"error": "ã™ã¹ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã§å‡¦ç†ã«å¤±æ•—ã—ã¾ã—ãŸ"}
    
    # å‡¦ç†æ¦‚è¦ï¼ˆè©³ç´°ï¼‰
    result = {
        "processing_summary": {
            "total_chunks": len(chunk_results),
            "successful_chunks": len([r for r in chunk_results if r["status"] == "success"]),
            "partial_chunks": len([r for r in chunk_results if r["status"] == "partial"]),
            "blocked_chunks": len(blocked),
            "failed_chunks": len(failed),
            "safety_filter_issues": len(blocked) > 0
        }
    }
    
    # å®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã®è­¦å‘Š
    if blocked:
        result["safety_warning"] = {
            "message": "ä¸€éƒ¨ã®ãƒãƒ£ãƒ³ã‚¯ãŒå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿ã«ã‚ˆã‚Šãƒ–ãƒ­ãƒƒã‚¯ã•ã‚Œã¾ã—ãŸ",
            "blocked_pages": [r["pages"] for r in blocked],
            "recommendation": "ã‚ˆã‚Šå°ã•ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºã¾ãŸã¯ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ã‚’è©¦ã—ã¦ãã ã•ã„"
        }
    
    # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒãƒ¼ã‚¸ï¼ˆæˆåŠŸãŠã‚ˆã³éƒ¨åˆ†æˆåŠŸã®ã¿ï¼‰
    for chunk in sorted(successful, key=lambda x: x["id"]):
        for key, value in chunk["data"].items():
            if key not in result:
                result[key] = value
            else:
                if isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = merge_dicts(result[key], value)
                elif isinstance(result[key], list) and isinstance(value, list):
                    result[key] = merge_items(result[key], value)
                elif isinstance(result[key], str) and isinstance(value, str):
                    result[key] = merge_text(result[key], value)
    
    # å…¨ä½“ã®é‡è¤‡é™¤å»å‡¦ç†
    result = clean_duplicates(result)
    
    return result

def clean_json_response(response):
    """AIãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆå¼·åŒ–ç‰ˆï¼‰"""
    if isinstance(response, dict):
        return response  # æ—¢ã«ãƒ‘ãƒ¼ã‚¹æ¸ˆã¿ã®å ´åˆ
    
    if not response:
        return {}
    
    try:
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’æ–‡å­—åˆ—ã«å¤‰æ›
        response_str = str(response).strip()
        
        # ç©ºã®å ´åˆ
        if not response_str:
            return {}
        
        # ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã‚³ãƒ¼ãƒ‰ãƒ–ãƒ­ãƒƒã‚¯ã®é™¤å»
        patterns_to_remove = [
            r'```json\s*',
            r'```\s*',
            r'^\s*json\s*',
            r'`+',
        ]
        
        for pattern in patterns_to_remove:
            response_str = re.sub(pattern, '', response_str, flags=re.IGNORECASE | re.MULTILINE)
        
        response_str = response_str.strip()
        
        # JSONã§ã¯ãªã„èª¬æ˜æ–‡ã®é™¤å»
        lines = response_str.split('\n')
        json_lines = []
        json_started = False
        
        for line in lines:
            stripped_line = line.strip()
            if stripped_line.startswith('{') or stripped_line.startswith('['):
                json_started = True
            if json_started:
                json_lines.append(line)
            if json_started and (stripped_line.endswith('}') or stripped_line.endswith(']')):
                break
        
        if json_lines:
            response_str = '\n'.join(json_lines)
        
        # ä¸æ­£ãªæ–‡å­—ã®ä¿®æ­£
        response_str = fix_json_syntax(response_str)
        
        # JSONã¨ã—ã¦è§£æ
        try:
            parsed = json.loads(response_str)
            return parsed
        except json.JSONDecodeError:
            # éƒ¨åˆ†çš„ãªJSONã®ä¿®å¾©ã‚’è©¦è¡Œ
            repaired = repair_partial_json(response_str)
            if repaired:
                return json.loads(repaired)
            
            # ãã‚Œã§ã‚‚å¤±æ•—ã—ãŸå ´åˆã¯åŸºæœ¬æ§‹é€ ã‚’è¿”ã™
            return create_basic_structure_from_text(response_str)
            
    except Exception as e:
        st.warning(f"JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        return {}

def fix_json_syntax(text):
    """JSONæ§‹æ–‡ã®ä¿®æ­£"""
    import re
    
    # ä¸€èˆ¬çš„ãªJSONæ§‹æ–‡ã‚¨ãƒ©ãƒ¼ã®ä¿®æ­£
    fixes = [
        # æœ«å°¾ã®ã‚«ãƒ³ãƒé™¤å»
        (r',(\s*[}\]])', r'\1'),
        # ä¸æ­£ãªã‚¯ã‚©ãƒ¼ãƒˆä¿®æ­£
        (r'([{,]\s*)([a-zA-Z_][a-zA-Z0-9_]*)\s*:', r'\1"\2":'),
        # å˜ä¸€ã‚¯ã‚©ãƒ¼ãƒˆã‚’ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã«
        (r"'([^']*)'", r'"\1"'),
        # åˆ¶å¾¡æ–‡å­—ã®é™¤å»
        (r'[\x00-\x1f\x7f-\x9f]', ''),
        # ä¸æ­£ãªæ”¹è¡Œã®ä¿®æ­£
        (r'"\s*\n\s*"', r'""'),
    ]
    
    result = text
    for pattern, replacement in fixes:
        result = re.sub(pattern, replacement, result, flags=re.MULTILINE)
    
    return result

def repair_partial_json(text):
    """éƒ¨åˆ†çš„ãªJSONã®ä¿®å¾©"""
    try:
        # ä¸å®Œå…¨ãªJSONã‚’å®Œæˆã•ã›ã‚‹è©¦è¡Œ
        text = text.strip()
        
        # é–‹å§‹æ–‡å­—ãƒã‚§ãƒƒã‚¯
        if not text.startswith(('{', '[')):
            return None
        
        # å¯¾å¿œã™ã‚‹æ‹¬å¼§ã®æ•°ã‚’ãƒã‚§ãƒƒã‚¯
        open_braces = text.count('{')
        close_braces = text.count('}')
        open_brackets = text.count('[')
        close_brackets = text.count(']')
        
        # ä¸è¶³ã—ã¦ã„ã‚‹é–‰ã˜æ‹¬å¼§ã‚’è¿½åŠ 
        if open_braces > close_braces:
            text += '}' * (open_braces - close_braces)
        
        if open_brackets > close_brackets:
            text += ']' * (open_brackets - close_brackets)
        
        # æœ«å°¾ã®ã‚«ãƒ³ãƒã‚„ä¸å®Œå…¨ãªè¦ç´ ã‚’ä¿®æ­£
        text = re.sub(r',\s*([}\]])', r'\1', text)
        text = re.sub(r':\s*([,}\]])', r': null\1', text)
        
        return text
    except:
        return None

def create_basic_structure_from_text(text):
    """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸºæœ¬çš„ãªJSONæ§‹é€ ã‚’ä½œæˆ"""
    try:
        # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åŸºæœ¬çš„ãªæƒ…å ±ã‚’æŠ½å‡º
        result = {
            "error": "JSONè§£æã«å¤±æ•—ã—ãŸãŸã‚åŸºæœ¬æ§‹é€ ã‚’ä½œæˆ",
            "raw_content": text[:500] + "..." if len(text) > 500 else text,
            "extraction_method": "fallback"
        }
        
        # å¯èƒ½ã§ã‚ã‚Œã°ä¸€äº›åŸºæœ¬æƒ…å ±ã‚’æŠ½å‡º
        lines = text.split('\n')
        for line in lines:
            # ç‰¹è¨±ç•ªå·ã‚‰ã—ãæ–‡å­—åˆ—ã‚’æ¤œç´¢
            if 'JP' in line and any(c.isdigit() for c in line):
                result["possible_patent_number"] = line.strip()
                break
        
        return result
    except:
        return {"error": "å®Œå…¨ãªJSONè§£æå¤±æ•—"}

def process_chunk(chunk, model_name, api_key, schema, prompt=None, temperature=0.1, max_tokens=32768):
    """ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ï¼ˆJSONãƒ‘ãƒ¼ã‚¹å•é¡Œã®æ ¹æœ¬è§£æ±ºï¼‰"""
    try:
        # ã‚ˆã‚Šç¢ºå®ŸãªJSONå‡ºåŠ›ã‚’ä¿ƒã™ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        robust_prompt = f"""
        ã‚ãªãŸã¯ç‰¹è¨±æ–‡æ›¸è§£æã®å°‚é–€å®¶ã§ã™ã€‚ä»¥ä¸‹ã®ç‰¹è¨±æ–‡æ›¸ã‹ã‚‰æƒ…å ±ã‚’æŠ½å‡ºã—ã€æœ‰åŠ¹ãªJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚

        é‡è¦ãªãƒ«ãƒ¼ãƒ«:
        1. å‡ºåŠ›ã¯å¿…ãšæœ‰åŠ¹ãªJSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§å§‹ã¾ã‚Šã€JSONã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã§çµ‚ã‚ã‚‹ã“ã¨
        2. æ–‡å­—åˆ—å€¤å†…ã®æ”¹è¡Œã¯ \\n ã§è¡¨ç¾ã™ã‚‹ã“ã¨
        3. ãƒ€ãƒ–ãƒ«ã‚¯ã‚©ãƒ¼ãƒˆã¯ \\" ã§ã‚¨ã‚¹ã‚±ãƒ¼ãƒ—ã™ã‚‹ã“ã¨
        4. èª¬æ˜ã‚„ã‚³ãƒ¡ãƒ³ãƒˆã¯å«ã‚ãšã€JSONãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’å‡ºåŠ›ã™ã‚‹ã“ã¨
        5. ä¸æ˜ãªé …ç›®ã¯ null ã¾ãŸã¯ç©ºæ–‡å­—åˆ—ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨

        å‡ºåŠ›ä¾‹:
        {{
            "title": "ç™ºæ˜ã®åç§°",
            "publication_number": "ç‰¹è¨±ç•ªå·",
            "abstract": "è¦ç´„æ–‡"
        }}

        {prompt or ""}
        
        å¿…ãšJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›ã—ã¦ãã ã•ã„:
        """
        
        extractor = PatentExtractor(model_name, api_key, schema, robust_prompt, temperature, max_tokens)
        
        # ã‚ˆã‚Šä½ã„æ¸©åº¦è¨­å®šã§ç¢ºå®šçš„ãªå‡ºåŠ›ã‚’ä¿ƒã™
        safe_temperature = min(0.0, temperature)
        extractor.temperature = safe_temperature
        
        raw_result = extractor.process_patent_pdf(chunk["path"])
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®è©³ç´°ãƒ­ã‚°ï¼ˆãƒ‡ãƒãƒƒã‚°ç”¨ï¼‰
        if isinstance(raw_result, str):
            st.write(f"ãƒ‡ãƒãƒƒã‚°: ãƒãƒ£ãƒ³ã‚¯{chunk['id']}ã®ç”Ÿãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆæœ€åˆã®200æ–‡å­—ï¼‰: {str(raw_result)[:200]}...")
        
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å®‰å…¨æ€§ãƒã‚§ãƒƒã‚¯
        if raw_result is None:
            return create_error_result(chunk, "ç©ºã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹")
        
        # JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨è§£æ
        cleaned_result = clean_json_response(raw_result)
        
        if not cleaned_result or cleaned_result == {}:
            return create_error_result(chunk, "JSONã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°å¤±æ•—")
        
        return {"id": chunk["id"], "pages": chunk["pages"], "status": "success", "data": cleaned_result}
        
    except Exception as e:
        error_msg = str(e)
        st.error(f"ãƒãƒ£ãƒ³ã‚¯{chunk['id']}å‡¦ç†ã‚¨ãƒ©ãƒ¼: {error_msg}")
        return create_error_result(chunk, error_msg)

def create_error_result(chunk, error_msg):
    """ã‚¨ãƒ©ãƒ¼æ™‚ã®çµæœä½œæˆ"""
    return {
        "id": chunk["id"], 
        "pages": chunk["pages"], 
        "status": "error", 
        "error": error_msg, 
        "data": {
            "error_info": {
                "chunk_id": chunk["id"],
                "pages": chunk["pages"],
                "error_message": error_msg
            }
        }
    }

# æ­£è¦è¡¨ç¾ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
import re

def safe_json_dumps(obj, **kwargs):
    """å®‰å…¨ãªJSONæ–‡å­—åˆ—åŒ–"""
    try:
        return json.dumps(obj, ensure_ascii=False, **kwargs)
    except (TypeError, ValueError) as e:
        # ã‚·ãƒªã‚¢ãƒ©ã‚¤ã‚ºã§ããªã„ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆãŒã‚ã‚‹å ´åˆã®å¯¾å‡¦
        st.warning(f"JSONå¤‰æ›ã‚¨ãƒ©ãƒ¼: {e}")
        return json.dumps({"error": "JSONå¤‰æ›ã«å¤±æ•—ã—ã¾ã—ãŸ"}, ensure_ascii=False, **kwargs)

def merge_text(text1, text2):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ãƒ»å®‰å…¨æ€§å‘ä¸Šï¼‰"""
    # å…¥åŠ›å€¤ã®å‹ãƒã‚§ãƒƒã‚¯ã¨å®‰å…¨ãªå¤‰æ›
    str1 = str(text1) if text1 is not None else ""
    str2 = str(text2) if text2 is not None else ""
    
    if not str1 or not str2:
        return str1 or str2
    
    str1, str2 = str1.strip(), str2.strip()
    
    # å®Œå…¨ä¸€è‡´ã®å ´åˆã¯ç‰‡æ–¹ã‚’è¿”ã™
    if str1 == str2:
        return str1
    
    # ä¸€æ–¹ãŒä»–æ–¹ã«å«ã¾ã‚Œã‚‹å ´åˆã¯é•·ã„æ–¹ã‚’è¿”ã™
    if str1 in str2:
        return str2
    if str2 in str1:
        return str1
    
    # ç©ºç™½åŒºåˆ‡ã‚Šã§åˆ†å‰²ã—ã¦é‡è¤‡ã‚’é™¤å»
    try:
        words1 = str1.split()
        words2 = str2.split()
        
        # å¤§éƒ¨åˆ†ãŒé‡è¤‡ã—ã¦ã„ã‚‹å ´åˆï¼ˆé¡ä¼¼åº¦80%ä»¥ä¸Šï¼‰ã¯é•·ã„æ–¹ã‚’æ¡ç”¨
        common_words = set(words1) & set(words2)
        total_words = set(words1) | set(words2)
        if total_words and len(common_words) / len(total_words) > 0.8:
            return str1 if len(str1) > len(str2) else str2
        
        # ç¶™ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        continues = (not str1.endswith(('.', 'ã€‚', '!', 'ï¼Ÿ')) or 
                    str1.endswith((',', 'ã€', ';')) or 
                    str2.startswith(('ãŒ', 'ã‚’', 'ã«', 'ã®', 'ã¯', 'ã¨', 'ã§')) or 
                    (str2 and str2[0].islower()))
        
        return f"{str1} {str2}" if continues else f"{str1}\n\n{str2}"
    except Exception:
        # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å®‰å…¨ã«çµåˆ
        return f"{str1} {str2}"

def merge_items(list1, list2):
    """ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ãƒ»ã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰"""
    if not isinstance(list1, list):
        list1 = []
    if not isinstance(list2, list):
        list2 = []
    
    if not list1:
        return list2
    if not list2:
        return list1
    
    result = list1.copy()
    
    for item in list2:
        is_duplicate = False
        
        try:
            for existing in result:
                if isinstance(item, dict) and isinstance(existing, dict):
                    # è¾æ›¸ã®å ´åˆï¼šã‚­ãƒ¼ã¨å€¤ã®çµ„ã¿åˆã‚ã›ã§åˆ¤å®š
                    if (item.get('id') and existing.get('id') and item['id'] == existing['id']) or \
                       (item.get('number') and existing.get('number') and item['number'] == existing['number']) or \
                       (item.get('content') and existing.get('content') and 
                        item.get('content', '') == existing.get('content', '')):
                        is_duplicate = True
                        # ã‚ˆã‚Šå®Œå…¨ãªæƒ…å ±ã§æ—¢å­˜ã‚’æ›´æ–°
                        if len(str(item)) > len(str(existing)):
                            idx = result.index(existing)
                            result[idx] = item
                        break
                elif isinstance(item, str) and isinstance(existing, str):
                    # æ–‡å­—åˆ—ã®å ´åˆï¼šå†…å®¹ã§åˆ¤å®š
                    if item == existing or item in existing or existing in item:
                        is_duplicate = True
                        # ã‚ˆã‚Šé•·ã„æ–‡å­—åˆ—ã§æ›´æ–°
                        if len(item) > len(existing):
                            idx = result.index(existing)
                            result[idx] = item
                        break
                elif str(item) == str(existing):
                    is_duplicate = True
                    break
        except Exception:
            # æ¯”è¼ƒã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯é‡è¤‡ã—ã¦ã„ãªã„ã¨ã¿ãªã™
            pass
        
        if not is_duplicate:
            result.append(item)
    
    return result

def merge_dicts(dict1, dict2):
    """è¾æ›¸ã‚’å†å¸°çš„ã«ãƒãƒ¼ã‚¸ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ï¼‰"""
    if not isinstance(dict1, dict):
        dict1 = {}
    if not isinstance(dict2, dict):
        dict2 = {}
    
    result = dict1.copy()
    
    for key, value in dict2.items():
        try:
            if key in result:
                if isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = merge_dicts(result[key], value)
                elif isinstance(result[key], list) and isinstance(value, list):
                    result[key] = merge_items(result[key], value)
                elif isinstance(result[key], str) and isinstance(value, str):
                    result[key] = merge_text(result[key], value)
                else:
                    # å‹ãŒç•°ãªã‚‹å ´åˆã¯æ–‡å­—åˆ—ã¨ã—ã¦çµåˆ
                    result[key] = merge_text(str(result[key]), str(value))
            else:
                result[key] = value
        except Exception as e:
            # ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯æ–°ã—ã„å€¤ã‚’æ¡ç”¨
            st.warning(f"ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼ (ã‚­ãƒ¼: {key}): {e}")
            result[key] = value
    
    return result

def merge_text(text1, text2):
    """ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ï¼‰"""
    if not text1 or not text2:
        return text1 or text2
    
    text1, text2 = text1.strip(), text2.strip()
    
    # å®Œå…¨ä¸€è‡´ã®å ´åˆã¯ç‰‡æ–¹ã‚’è¿”ã™
    if text1 == text2:
        return text1
    
    # ä¸€æ–¹ãŒä»–æ–¹ã«å«ã¾ã‚Œã‚‹å ´åˆã¯é•·ã„æ–¹ã‚’è¿”ã™
    if text1 in text2:
        return text2
    if text2 in text1:
        return text1
    
    # ç©ºç™½åŒºåˆ‡ã‚Šã§åˆ†å‰²ã—ã¦é‡è¤‡ã‚’é™¤å»
    words1 = text1.split()
    words2 = text2.split()
    
    # å¤§éƒ¨åˆ†ãŒé‡è¤‡ã—ã¦ã„ã‚‹å ´åˆï¼ˆé¡ä¼¼åº¦80%ä»¥ä¸Šï¼‰ã¯é•·ã„æ–¹ã‚’æ¡ç”¨
    common_words = set(words1) & set(words2)
    total_words = set(words1) | set(words2)
    if total_words and len(common_words) / len(total_words) > 0.8:
        return text1 if len(text1) > len(text2) else text2
    
    # ç¶™ç¶šãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
    continues = (not text1.endswith(('.', 'ã€‚', '!', 'ï¼Ÿ')) or 
                text1.endswith((',', 'ã€', ';')) or 
                text2.startswith(('ãŒ', 'ã‚’', 'ã«', 'ã®', 'ã¯', 'ã¨', 'ã§')) or 
                (text2 and text2[0].islower()))
    
    return f"{text1} {text2}" if continues else f"{text1}\n\n{text2}"

def merge_items(list1, list2):
    """ãƒªã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã‚’ãƒãƒ¼ã‚¸ï¼ˆé‡è¤‡é™¤å»ï¼‰"""
    if not list1:
        return list2
    if not list2:
        return list1
    
    result = list1.copy()
    
    for item in list2:
        # ã‚¢ã‚¤ãƒ†ãƒ ã®é‡è¤‡ãƒã‚§ãƒƒã‚¯
        is_duplicate = False
        
        for existing in result:
            if isinstance(item, dict) and isinstance(existing, dict):
                # è¾æ›¸ã®å ´åˆï¼šã‚­ãƒ¼ã¨å€¤ã®çµ„ã¿åˆã‚ã›ã§åˆ¤å®š
                if ('id' in item and 'id' in existing and item['id'] == existing['id']) or \
                   ('number' in item and 'number' in existing and item['number'] == existing['number']) or \
                   ('content' in item and 'content' in existing and 
                    item.get('content', '') == existing.get('content', '')):
                    is_duplicate = True
                    # ã‚ˆã‚Šå®Œå…¨ãªæƒ…å ±ã§æ—¢å­˜ã‚’æ›´æ–°
                    if len(str(item)) > len(str(existing)):
                        idx = result.index(existing)
                        result[idx] = item
                    break
            elif isinstance(item, str) and isinstance(existing, str):
                # æ–‡å­—åˆ—ã®å ´åˆï¼šå†…å®¹ã§åˆ¤å®š
                if item == existing or item in existing or existing in item:
                    is_duplicate = True
                    # ã‚ˆã‚Šé•·ã„æ–‡å­—åˆ—ã§æ›´æ–°
                    if len(item) > len(existing):
                        idx = result.index(existing)
                        result[idx] = item
                    break
            elif str(item) == str(existing):
                is_duplicate = True
                break
        
        if not is_duplicate:
            result.append(item)
    
    return result

def clean_duplicates(data):
    """ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã‹ã‚‰é‡è¤‡ã‚’é™¤å»ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–å¼·åŒ–ï¼‰"""
    try:
        if isinstance(data, dict):
            cleaned = {}
            for key, value in data.items():
                try:
                    if isinstance(value, str):
                        # æ–‡å­—åˆ—ã‹ã‚‰é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
                        cleaned_text = remove_repeated_patterns(value)
                        cleaned[key] = cleaned_text
                    elif isinstance(value, (dict, list)):
                        cleaned[key] = clean_duplicates(value)
                    else:
                        cleaned[key] = value
                except Exception as e:
                    # å€‹åˆ¥ã‚­ãƒ¼ã®å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ãŸå ´åˆã¯å…ƒã®å€¤ã‚’ä¿æŒ
                    st.warning(f"ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼ (ã‚­ãƒ¼: {key}): {e}")
                    cleaned[key] = value
            return cleaned
        elif isinstance(data, list):
            try:
                return [clean_duplicates(item) for item in data]
            except Exception:
                # ãƒªã‚¹ãƒˆå‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒªã‚¹ãƒˆã‚’è¿”ã™
                return data
        else:
            return data
    except Exception:
        # å…¨ä½“å‡¦ç†ã§ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿”ã™
        return data

def remove_repeated_patterns(text):
    """æ–‡å­—åˆ—ã‹ã‚‰é‡è¤‡ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»ï¼ˆã‚¨ãƒ©ãƒ¼å¯¾ç­–ï¼‰"""
    try:
        if not isinstance(text, str) or not text.strip():
            return text
        
        # ç©ºç™½ã§åˆ†å‰²
        words = text.split()
        if len(words) <= 1:
            return text
        
        # é€£ç¶šã™ã‚‹é‡è¤‡å˜èªã‚’é™¤å»
        cleaned_words = [words[0]]
        for word in words[1:]:
            if word != cleaned_words[-1]:
                cleaned_words.append(word)
        
        # åŒã˜ãƒ•ãƒ¬ãƒ¼ã‚ºã®ç¹°ã‚Šè¿”ã—ã‚’æ¤œå‡ºã—ã¦é™¤å»
        result_text = ' '.join(cleaned_words)
        
        # ã‚ˆã‚Šé•·ã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é‡è¤‡ã‚’æ¤œå‡º
        for pattern_length in range(min(len(cleaned_words) // 2, 10), 0, -1):
            try:
                pattern = cleaned_words[:pattern_length]
                pattern_str = ' '.join(pattern)
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒè¤‡æ•°å›ç¹°ã‚Šè¿”ã•ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
                if len(pattern_str) > 0 and result_text.count(pattern_str) > 1:
                    # æœ€åˆã®å‡ºç¾ã®ã¿ã‚’æ®‹ã™
                    parts = result_text.split(pattern_str)
                    if len(parts) > 2:  # ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒ2å›ä»¥ä¸Šå‡ºç¾
                        result_text = pattern_str.join([parts[0], parts[1]]) + pattern_str
                        break
            except Exception:
                continue
        
        return result_text.strip()
    except Exception:
        # ã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å…ƒã®ãƒ†ã‚­ã‚¹ãƒˆã‚’è¿”ã™
        return str(text) if text is not None else ""

def merge_dicts(dict1, dict2):
    """è¾æ›¸ã‚’å†å¸°çš„ã«ãƒãƒ¼ã‚¸"""
    result = dict1.copy()
    for key, value in dict2.items():
        if key in result:
            if isinstance(result[key], dict) and isinstance(value, dict):
                result[key] = merge_dicts(result[key], value)
            elif isinstance(result[key], list) and isinstance(value, list):
                result[key] = merge_items(result[key], value)
            elif isinstance(result[key], str) and isinstance(value, str):
                result[key] = merge_text(result[key], value)
        else:
            result[key] = value
    return result

def merge_chunks(chunk_results):
    """ãƒãƒ£ãƒ³ã‚¯çµæœã‚’çµ±åˆï¼ˆå®‰å…¨æ€§ãƒ•ã‚£ãƒ«ã‚¿å¯¾å¿œï¼‰"""
    return merge_chunks_with_safety_handling(chunk_results)

def process_pdf(pdf_path, model_name, api_key, schema, prompt=None, temperature=0.1, 
               max_tokens=32768, chunk_size=2, overlap=1, progress_container=None):
    """PDFå…¨ä½“ã‚’å‡¦ç†"""
    chunks = split_pdf_chunks(pdf_path, chunk_size, overlap)
    if not chunks:
        return {"error": "PDFåˆ†å‰²å¤±æ•—"}
    
    results = []
    total = len(chunks)
    
    if progress_container:
        progress_bar = progress_container.progress(0)
        status_text = progress_container.empty()
        badge_container = progress_container.empty()
    
    try:
        for i, chunk in enumerate(chunks):
            if progress_container:
                progress_bar.progress((i + 1) / total)
                status_text.text(f"ãƒãƒ£ãƒ³ã‚¯ {chunk['id']}/{total} (P{chunk['start']}-{chunk['end']}) å‡¦ç†ä¸­...")
            
            result = process_chunk(chunk, model_name, api_key, schema, prompt, temperature, max_tokens)
            results.append(result)
            
            # ãƒãƒƒã‚¸è¡¨ç¤ºæ›´æ–°
            if progress_container:
                badges = []
                for j, res in enumerate(results):
                    pages = f"P{res['pages'][0]}-{res['pages'][-1]}"
                    status_class = "page-success" if res["status"] == "success" else "page-error"
                    symbol = "âœ“" if res["status"] == "success" else "âœ—"
                    badges.append(f'<span class="page-badge {status_class}">C{res["id"]} ({pages}) {symbol}</span>')
                
                # æœªå‡¦ç†ãƒãƒ£ãƒ³ã‚¯
                for k in range(len(results), total):
                    chunk_info = chunks[k]
                    pages = f"P{chunk_info['start']}-{chunk_info['end']}"
                    class_name = "page-processing" if k == len(results) else ""
                    symbol = "..." if k == len(results) else ""
                    badges.append(f'<span class="page-badge {class_name}">C{chunk_info["id"]} ({pages}) {symbol}</span>')
                
                badge_container.markdown(f'<div>{"".join(badges)}</div>', unsafe_allow_html=True)
        
        if progress_container:
            status_text.text("å‡¦ç†å®Œäº†")
    
    finally:
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤
        for chunk in chunks:
            try:
                os.remove(chunk["path"])
            except:
                pass
    
    return merge_chunks(results)

# UI
st.title("ğŸ§© ç‰¹è¨±PDFæ§‹é€ åŒ–ãƒ„ãƒ¼ãƒ«")
st.markdown("ç‰¹è¨±PDFã‹ã‚‰ãƒãƒ«ãƒãƒ¢ãƒ¼ãƒ€ãƒ«ç”ŸæˆAIã‚’ä½¿ç”¨ã—ã¦æ§‹é€ åŒ–JSONã‚’æŠ½å‡ºã—ã¾ã™ã€‚")

# ã‚µã‚¤ãƒ‰ãƒãƒ¼
with st.sidebar:
    st.header("âš™ï¸ è¨­å®š")
    
    # ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼é¸æŠ
    provider = st.selectbox("AIãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼", list(DEFAULT_MODELS.keys()))
    
    # APIã‚­ãƒ¼
    api_key_env = {"Google Gemini": "GOOGLE_API_KEY", "OpenAI": "OPENAI_API_KEY", "Anthropic": "ANTHROPIC_API_KEY"}
    api_key = st.text_input(f"{provider} APIã‚­ãƒ¼", value=os.environ.get(api_key_env[provider], ""), type="password")
    
    # ãƒ¢ãƒ‡ãƒ«é¸æŠ
    available_models = get_models(provider, api_key)
    model_name = st.selectbox("ãƒ¢ãƒ‡ãƒ«", available_models) if available_models else ""
    
    # ã‚¹ã‚­ãƒ¼ãƒ
    schema_type = st.radio("JSONã‚¹ã‚­ãƒ¼ãƒ", ["ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ", "ãƒ•ã‚¡ã‚¤ãƒ«", "ç›´æ¥å…¥åŠ›"])
    schema = {}
    
    if schema_type == "ãƒ•ã‚¡ã‚¤ãƒ«":
        uploaded_schema = st.file_uploader("JSONã‚¹ã‚­ãƒ¼ãƒãƒ•ã‚¡ã‚¤ãƒ«", type=["json"])
        if uploaded_schema:
            schema = load_schema(content=uploaded_schema.getvalue().decode("utf-8"))
    elif schema_type == "ç›´æ¥å…¥åŠ›":
        schema_text = st.text_area("JSONã‚¹ã‚­ãƒ¼ãƒ", height=150)
        if schema_text:
            schema = load_schema(content=schema_text)
    else:
        schema = load_schema("default_schema.json")
    
    # è©³ç´°è¨­å®š
    with st.expander("è©³ç´°è¨­å®š"):
        prompt = st.text_area("ã‚«ã‚¹ã‚¿ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ", height=80)
        temperature = st.slider("Temperature", 0.0, 1.0, 0.1, 0.05)
        max_tokens = st.number_input("æœ€å¤§ãƒˆãƒ¼ã‚¯ãƒ³æ•°", 1024, 65535, 32768, 1024)
        
        st.markdown("### ãƒãƒ£ãƒ³ã‚¯è¨­å®š")
        chunk_size = st.selectbox("ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º", [1, 2, 3, 4, 5], index=1)
        overlap = st.selectbox("é‡è¤‡ã‚µã‚¤ã‚º", [0, 1, 2], index=1)

# ãƒ¡ã‚¤ãƒ³
col1, col2 = st.columns(2)

with col1:
    st.header("ğŸ“¤ å…¥åŠ›")
    uploaded_pdf = st.file_uploader("ç‰¹è¨±PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["pdf"])
    
    if uploaded_pdf:
        st.success(f"ãƒ•ã‚¡ã‚¤ãƒ«: {uploaded_pdf.name}")
        try:
            temp_path = tempfile.mktemp(suffix=".pdf")
            with open(temp_path, "wb") as f:
                f.write(uploaded_pdf.getbuffer())
            
            reader = PdfReader(temp_path)
            st.info(f"ğŸ“„ ç·ãƒšãƒ¼ã‚¸æ•°: {len(reader.pages)}")
            os.remove(temp_path)
        except:
            pass
    
    process_button = st.button("å‡¦ç†é–‹å§‹", disabled=not (uploaded_pdf and api_key and model_name))

with col2:
    st.header("ğŸ“¥ å‡ºåŠ›")
    
    if process_button and uploaded_pdf and api_key and model_name:
        with st.status("å‡¦ç†ä¸­...", expanded=True) as status:
            # PDFä¿å­˜
            pdf_path = tempfile.mktemp(suffix=".pdf")
            with open(pdf_path, "wb") as f:
                f.write(uploaded_pdf.getbuffer())
            
            # å‡¦ç†å®Ÿè¡Œ
            progress_container = st.container()
            result = process_pdf(pdf_path, model_name, api_key, schema, prompt, 
                               temperature, max_tokens, chunk_size, overlap, progress_container)
            
            # å¾Œå‡¦ç†
            os.remove(pdf_path)
            
            if "error" in result:
                status.update(label=f"ã‚¨ãƒ©ãƒ¼: {result['error']}", state="error")
            else:
                status.update(label="å‡¦ç†å®Œäº†", state="complete")
                
                # çµ±è¨ˆè¡¨ç¤º
                summary = result.get("processing_summary", {})
                col1, col2, col3 = st.columns(3)
                with col1:
                    st.markdown(f'<div class="stat-box blue-stat"><h3>{summary.get("total_chunks", 0)}</h3>ç·ãƒãƒ£ãƒ³ã‚¯</div>', unsafe_allow_html=True)
                with col2:
                    st.markdown(f'<div class="stat-box green-stat"><h3>{summary.get("successful_chunks", 0)}</h3>æˆåŠŸ</div>', unsafe_allow_html=True)
                with col3:
                    st.markdown(f'<div class="stat-box orange-stat"><h3>{summary.get("failed_chunks", 0)}</h3>å¤±æ•—</div>', unsafe_allow_html=True)
                
                # çµæœè¡¨ç¤º
                display_data = {k: v for k, v in result.items() if k != "processing_summary"}
                st.markdown("### JSONå‡ºåŠ›")
                st.json(display_data)
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
                json_str = safe_json_dumps(result, indent=2)
                filename = f"{Path(uploaded_pdf.name).stem}_processed.json"
                st.download_button("ğŸ“¥ JSONãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰", json_str.encode("utf-8"), filename, "application/json")
    else:
        st.info("PDFã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦å‡¦ç†ã‚’é–‹å§‹ã—ã¦ãã ã•ã„")

st.markdown("---")
st.markdown("""
<div style="text-align: center; color: #666;">
ç‰¹è¨±PDFæ§‹é€ åŒ–ãƒ„ãƒ¼ãƒ« - ãƒãƒ£ãƒ³ã‚¯é‡è¤‡å‡¦ç†ã«ã‚ˆã‚‹AIæ–‡æ›¸è§£æ<br>
<small>Powered by patent-extractor library</small>
</div>
""", unsafe_allow_html=True)